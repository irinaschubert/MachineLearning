{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# define project path\n",
    "projectpath = 'projectpath/'\n",
    "train_data_csv = projectpath + 'ready_train.csv'\n",
    "test_data_csv = projectpath + 'ready_test.csv'\n",
    "\n",
    "df_train = pd.read_csv(train_data_csv,header=None)\n",
    "\n",
    "# save target values with index into new dataframe 'labels'\n",
    "df_labels = pd.read_csv(train_data_csv, names=['target'], header=None, usecols=[1])\n",
    "\n",
    "# save as array\n",
    "labels = df_labels['target'].values\n",
    "\n",
    "# save data into new dataframe 'training_data'\n",
    "df_training_data_raw = df_train.drop(columns=[0,1])\n",
    "\n",
    "# split dataset into train (80%) and test data (20%); random_state=1 makes it reproducible (could be any number); stratify ensures that the proportion stays in test and training data sets\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(df_training_data_raw, labels, shuffle=True, test_size=0.2, random_state=1, stratify=df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['G', 'F', 'G', 'J', 'G'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no gridsearchcv necessary so just create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model to data\n",
    "optimal_model.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## see how model performs on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G' 'F' 'E' 'J' 'E' 'J' 'B' 'D' 'C' 'J' 'J' 'C' 'G' 'C' 'G' 'C' 'H' 'E'\n",
      " 'D' 'C' 'I' 'H' 'C' 'B' 'A' 'B' 'H' 'B' 'E' 'H' 'H' 'I' 'J' 'D' 'I' 'F'\n",
      " 'J' 'H' 'A' 'B' 'E' 'H' 'I' 'I' 'I' 'D' 'A' 'H' 'G' 'G' 'F' 'G' 'A' 'J'\n",
      " 'B' 'D' 'B' 'I' 'B' 'E' 'H' 'H' 'A' 'A' 'I' 'C' 'A' 'J' 'B' 'J' 'A' 'H'\n",
      " 'E' 'I' 'H' 'H' 'F' 'J' 'D' 'E' 'G' 'A' 'B' 'G' 'I' 'C' 'F' 'B' 'A' 'H'\n",
      " 'A' 'B' 'F' 'I' 'J' 'F' 'J' 'J' 'J' 'C']\n"
     ]
    }
   ],
   "source": [
    "# predict the labels for the test slice\n",
    "labels_pred = optimal_model.predict(data_test)\n",
    "print(labels_pred[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7618856837606838\n",
      "Precision: [0.77842566 0.6590621  0.87624467 0.87280702 0.85668277 0.79880952\n",
      " 0.86363636 0.77401894 0.62043796 0.62854251]\n",
      "Recall: [0.7129506  0.69425901 0.82352941 0.79706275 0.71028037 0.89586115\n",
      " 0.71028037 0.76368491 0.68090788 0.8302139 ]\n"
     ]
    }
   ],
   "source": [
    "# check the accuracy for the test data\n",
    "print(\"Accuracy:\", metrics.accuracy_score(labels_test, labels_pred))\n",
    "# check the precision for the test data\n",
    "print(\"Precision:\",metrics.precision_score(labels_test, labels_pred, average=None))\n",
    "# check the recall for the test data\n",
    "print(\"Recall:\",metrics.recall_score(labels_test, labels_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for all Naive Bayes Algorithms there are in scikitlearn..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (Multinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model = MultinomialNB()\n",
    "optimal_model.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G' 'F' 'E' 'J' 'I' 'I' 'B' 'D' 'G' 'J' 'J' 'C' 'G' 'C' 'G' 'C' 'H' 'E'\n",
      " 'D' 'G' 'I' 'H' 'C' 'G' 'A' 'B' 'H' 'B' 'E' 'H' 'H' 'I' 'J' 'D' 'I' 'F'\n",
      " 'J' 'H' 'A' 'B' 'E' 'H' 'I' 'I' 'I' 'D' 'A' 'H' 'C' 'G' 'I' 'G' 'B' 'J'\n",
      " 'B' 'D' 'B' 'I' 'B' 'E' 'H' 'H' 'A' 'A' 'B' 'C' 'A' 'J' 'B' 'J' 'B' 'H'\n",
      " 'E' 'I' 'H' 'H' 'F' 'J' 'D' 'F' 'G' 'B' 'D' 'G' 'I' 'E' 'F' 'B' 'B' 'H'\n",
      " 'A' 'E' 'F' 'I' 'J' 'A' 'I' 'I' 'J' 'G']\n"
     ]
    }
   ],
   "source": [
    "# predict the labels for the test slice\n",
    "labels_pred = optimal_model.predict(data_test)\n",
    "print(labels_pred[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7988782051282052\n",
      "Precision: [0.89123377 0.74484536 0.90882353 0.85074627 0.89983845 0.87931034\n",
      " 0.8757485  0.80726257 0.54591837 0.78418231]\n",
      "Recall: [0.7329773  0.77169559 0.82620321 0.83711615 0.74365821 0.88518024\n",
      " 0.78104139 0.77169559 0.85714286 0.78208556]\n"
     ]
    }
   ],
   "source": [
    "# check the accuracy for the test data\n",
    "print(\"Accuracy:\", metrics.accuracy_score(labels_test, labels_pred))\n",
    "# check the precision for the test data\n",
    "print(\"Precision:\",metrics.precision_score(labels_test, labels_pred, average=None))\n",
    "# check the recall for the test data\n",
    "print(\"Recall:\",metrics.recall_score(labels_test, labels_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "has best result, so use for test data and make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A' 'I' 'A' 'H' 'I' 'B' 'H' 'I' 'J' 'H' 'J' 'J' 'A' 'D' 'A' 'A' 'A' 'A'\n",
      " 'A' 'A' 'J' 'A' 'A' 'I' 'A' 'A' 'A' 'A' 'A' 'A' 'B' 'A' 'A' 'J' 'G' 'H'\n",
      " 'H' 'H' 'A' 'A' 'A' 'A' 'A' 'J' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A'\n",
      " 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'A' 'J' 'J' 'J' 'A' 'A' 'A' 'A' 'A' 'A' 'A'\n",
      " 'A' 'A' 'B' 'A' 'A' 'A' 'J' 'A' 'A' 'I' 'E' 'J' 'H' 'A' 'A' 'A' 'A' 'G'\n",
      " 'A' 'A' 'G' 'A' 'A' 'A' 'A' 'A' 'A' 'A']\n"
     ]
    }
   ],
   "source": [
    "df_test=pd.read_csv(test_data_csv, sep=',', header=None)\n",
    "test_data_new = df_test.drop(columns=[0])\n",
    "# predict values for new data\n",
    "predicted = optimal_model.predict(test_data_new)\n",
    "print(predicted[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission = pd.DataFrame(columns=['id','target'])\n",
    "predicted = pd.Series(predicted)\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id target\n",
       "0   0      A\n",
       "1   1      I\n",
       "2   2      A\n",
       "3   3      H\n",
       "4   4      I"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add calculated target values to csv and format for submission\n",
    "df_submission['target'] = predicted\n",
    "df_submission['id'] = df_submission.index\n",
    "submission_file = projectpath + 'submission_naive_bayes.csv'\n",
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(submission_file, index=False, columns=['id','target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (Complement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB(alpha=1.0, class_prior=None, fit_prior=True, norm=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model = ComplementNB()\n",
    "optimal_model.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I' 'F' 'I' 'J' 'J' 'I' 'B' 'D' 'C' 'J' 'I' 'C' 'G' 'C' 'C' 'C' 'H' 'C'\n",
      " 'D' 'C' 'I' 'H' 'C' 'G' 'A' 'J' 'H' 'A' 'F' 'H' 'H' 'I' 'J' 'D' 'I' 'F'\n",
      " 'J' 'H' 'A' 'H' 'E' 'H' 'I' 'I' 'I' 'D' 'A' 'H' 'C' 'J' 'I' 'G' 'J' 'J'\n",
      " 'B' 'D' 'B' 'A' 'H' 'C' 'H' 'H' 'A' 'A' 'I' 'C' 'I' 'I' 'A' 'D' 'J' 'H'\n",
      " 'I' 'I' 'H' 'H' 'I' 'D' 'D' 'F' 'G' 'A' 'G' 'G' 'I' 'F' 'F' 'D' 'I' 'H'\n",
      " 'A' 'E' 'I' 'I' 'J' 'A' 'I' 'I' 'J' 'D']\n"
     ]
    }
   ],
   "source": [
    "# predict the labels for the test slice\n",
    "labels_pred = optimal_model.predict(data_test)\n",
    "print(labels_pred[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6971153846153846\n",
      "Precision: [0.7994012  0.98305085 0.73684211 0.68627451 0.96261682 0.71210341\n",
      " 0.82748092 0.81656805 0.43384224 0.6550152 ]\n",
      "Recall: [0.7129506  0.38718291 0.86096257 0.8411215  0.41255007 0.80907877\n",
      " 0.72363151 0.73698264 0.9105474  0.57620321]\n"
     ]
    }
   ],
   "source": [
    "# check the accuracy for the test data\n",
    "print(\"Accuracy:\", metrics.accuracy_score(labels_test, labels_pred))\n",
    "# check the precision for the test data\n",
    "print(\"Precision:\",metrics.precision_score(labels_test, labels_pred, average=None))\n",
    "# check the recall for the test data\n",
    "print(\"Recall:\",metrics.recall_score(labels_test, labels_pred, average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes (Bernoulli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_model = BernoulliNB()\n",
    "optimal_model.fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G' 'F' 'E' 'J' 'E' 'J' 'B' 'D' 'C' 'J' 'J' 'C' 'G' 'C' 'G' 'C' 'H' 'E'\n",
      " 'D' 'C' 'I' 'H' 'C' 'G' 'A' 'B' 'H' 'B' 'E' 'H' 'H' 'I' 'J' 'D' 'I' 'F'\n",
      " 'J' 'H' 'A' 'B' 'E' 'H' 'I' 'I' 'I' 'D' 'A' 'H' 'C' 'G' 'A' 'G' 'J' 'J'\n",
      " 'B' 'D' 'B' 'I' 'B' 'E' 'H' 'H' 'A' 'A' 'B' 'C' 'A' 'J' 'B' 'J' 'A' 'H'\n",
      " 'E' 'I' 'H' 'H' 'F' 'J' 'D' 'F' 'G' 'A' 'B' 'G' 'I' 'E' 'F' 'B' 'G' 'H'\n",
      " 'A' 'E' 'F' 'I' 'J' 'A' 'J' 'J' 'J' 'C']\n"
     ]
    }
   ],
   "source": [
    "# predict the labels for the test slice\n",
    "labels_pred = optimal_model.predict(data_test)\n",
    "print(labels_pred[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7585470085470085\n",
      "Precision: [0.75397974 0.64030612 0.82526882 0.8774584  0.85572139 0.83121827\n",
      " 0.87025316 0.82378855 0.63409091 0.60839844]\n",
      "Recall: [0.69559413 0.67022697 0.82085561 0.77436582 0.68891856 0.87449933\n",
      " 0.73431242 0.74899866 0.74499332 0.8328877 ]\n"
     ]
    }
   ],
   "source": [
    "# check the accuracy for the test data\n",
    "print(\"Accuracy:\", metrics.accuracy_score(labels_test, labels_pred))\n",
    "# check the precision for the test data\n",
    "print(\"Precision:\",metrics.precision_score(labels_test, labels_pred, average=None))\n",
    "# check the recall for the test data\n",
    "print(\"Recall:\",metrics.recall_score(labels_test, labels_pred, average=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v_conda",
   "language": "python",
   "name": "v_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
